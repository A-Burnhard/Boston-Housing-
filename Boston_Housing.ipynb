{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQogcWCX/pk6CGJdrzvMu2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Burnhard/Boston-Housing-/blob/main/Boston_Housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn_BGAjeIa8C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regression_data = pd.read_csv('housing.csv')\n",
        "regression_data.head()"
      ],
      "metadata": {
        "id": "8cWfach7J62Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-processing**"
      ],
      "metadata": {
        "id": "rvvCueQwQHQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing values\n",
        "missing_values = regression_data.isnull().sum()\n",
        "print(\"Missing values:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "bCtb1CpFQMyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with mean or median if any\n",
        "regression_data.fillna(regression_data.mean(), inplace=True)"
      ],
      "metadata": {
        "id": "unWQwJ5LQUsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifying Duplicates**"
      ],
      "metadata": {
        "id": "DgQlKtqcovhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify duplicate rows\n",
        "duplicates = regression_data.duplicated()\n",
        "print(\"Duplicates instances: \\n\",duplicates)"
      ],
      "metadata": {
        "id": "SYxI71MDQaMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outlier detection**"
      ],
      "metadata": {
        "id": "3M3xBNuqozH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = iris_data\n",
        "\n",
        "# Visualize the distribution of each feature using box plots\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Boxplot of Features')\n",
        "plt.show()\n",
        "\n",
        "# Identify outliers using statistical methods (e.g., Z-score or IQR)\n",
        "# Z-score method\n",
        "from scipy.stats import zscore\n",
        "\n",
        "data = iris_data.drop(\"class\", axis=1)\n",
        "z_scores = zscore(data)\n",
        "outlier_threshold = 3  # Adjust the threshold as per your preference\n",
        "outliers = (abs(z_scores) > outlier_threshold).any(axis=1)\n",
        "\n",
        "# IQR method\n",
        "Q1 = data.quantile(0.25)\n",
        "Q3 = data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "\n",
        "# Count the number of outliers\n",
        "num_outliers = outliers.sum()\n",
        "print(f\"Number of outliers: {num_outliers}\")\n",
        "\n",
        "# Decide whether to remove outliers or transform them\n",
        "remove_outliers = False\n",
        "\n",
        "if remove_outliers:\n",
        "    # Remove outliers from the dataset\n",
        "    data = data[~outliers]\n",
        "    print(\"Outliers removed.\")\n",
        "else:\n",
        "    # Transform outliers to a specific value\n",
        "    outlier_value = 8  # Choose an appropriate value for transformation\n",
        "    data[outliers] = outlier_value\n",
        "    print(\"Outliers transformed.\")\n",
        "\n",
        "# Updated visualization after handling outliers\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Boxplot of Features (After Outlier Handling)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jDlJ7bUWo3UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining X and Y values**"
      ],
      "metadata": {
        "id": "tc9XzsVUpbaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the target variable (class) from the features\n",
        "iris_data = pd.read_csv(\"iris.data\", names=cols)\n",
        "\n",
        "X = iris_data.drop('class', axis=1)\n",
        "y = iris_data['class']\n",
        "\n",
        "# Convert the target variable to numeric labels\n",
        "label_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
        "y = y.map(label_mapping)"
      ],
      "metadata": {
        "id": "umldgnH6pckB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Influencial datapoint detection using leverage and cooks distance**"
      ],
      "metadata": {
        "id": "V5-Y4Bsho_zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add a constant term to the features matrix for the intercept in the linear regression model\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X)\n",
        "results = model.fit()\n",
        "\n",
        "# Get the influence statistics\n",
        "influence = OLSInfluence(results)\n",
        "\n",
        "# Calculate the leverage values\n",
        "leverage = influence.hat_matrix_diag\n",
        "\n",
        "# Calculate the Cook's distance\n",
        "cooks_distance = influence.cooks_distance\n",
        "\n",
        "# Identify influential data points based on leverage or Cook's distance\n",
        "influential_points_leverage = leverage > 2 * (X.shape[1] + 1) / X.shape[0]\n",
        "influential_points_cooks = cooks_distance[0] > 4 / (X.shape[0] - X.shape[1] - 1)\n",
        "\n",
        "# Print the influential data points\n",
        "print(\"Influential points based on leverage:\")\n",
        "print(X[influential_points_leverage])\n",
        "\n",
        "print(\"\\nInfluential points based on Cook's distance:\")\n",
        "print(X[influential_points_cooks])\n"
      ],
      "metadata": {
        "id": "2EhzLaHEpEzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normality of the set of features using shapiro**"
      ],
      "metadata": {
        "id": "V4tokeizpoeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "\n",
        "\n",
        "# Select the features to check for normality\n",
        "features = X\n",
        "\n",
        "# Perform Shapiro-Wilk test for each feature\n",
        "for column in features.columns:\n",
        "    stat, p_value = shapiro(features[column])\n",
        "    alpha = 0.05  # Significance level\n",
        "\n",
        "    print(f\"Feature: {column}\")\n",
        "    print(f\"Shapiro-Wilk test statistic: {stat}\")\n",
        "    print(f\"P-value: {p_value}\")\n",
        "\n",
        "    if p_value > alpha:\n",
        "        print(\"Feature appears to be normally distributed.\")\n",
        "    else:\n",
        "        print(\"Feature does not appear to be normally distributed.\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "93NTj2DAppzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Transformation**"
      ],
      "metadata": {
        "id": "qaK2s0-Ypu6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "X,y\n",
        "\n",
        "# Separate the target variable (class) from the features\n",
        "\n",
        "# Perform normalization using Min-Max scaler\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Perform standardization using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "9xfQrbapp0PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "jDR0USwAp3fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# Create a Random Forest regressor\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Fit the Random Forest model\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importances\n",
        "feature_importances_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the features by importance (descending order)\n",
        "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importances\n",
        "print(feature_importances_df)"
      ],
      "metadata": {
        "id": "vtFPJcyRp8Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Oversampling techniques using the Synthetic Minority Over-sampling Technique (SMOTE) to balance the imbalanced dataset**"
      ],
      "metadata": {
        "id": "kAL3cCMIqA9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE()\n",
        "\n",
        "# Perform oversampling\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Print the balanced class distribution\n",
        "print(\"Class distribution after SMOTE:\")\n",
        "print(y_resampled.value_counts())"
      ],
      "metadata": {
        "id": "-g1_2-18qIrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selecting Appropriate Learners for Training and Validation (Decision trees  and Gradient boosting)**"
      ],
      "metadata": {
        "id": "jjAzemxSqLrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import KFold, LeaveOneOut, train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "data = load_boston()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Define the models\n",
        "decision_tree_model = DecisionTreeRegressor(random_state=42)\n",
        "xgboost_model = XGBRegressor(random_state=42)\n",
        "\n",
        "# Function to perform K-fold cross-validation\n",
        "def k_fold_cross_validation(model, X, y, k):\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    mse_scores = []\n",
        "    for train_index, val_index in kf.split(X):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        mse = mean_squared_error(y_val, y_pred)\n",
        "        mse_scores.append(mse)\n",
        "\n",
        "    return np.mean(mse_scores)\n",
        "\n",
        "# Function to perform Leave-One-Out cross-validation\n",
        "def leave_one_out_cross_validation(model, X, y):\n",
        "    loo = LeaveOneOut()\n",
        "    mse_scores = []\n",
        "    for train_index, val_index in loo.split(X):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        mse = mean_squared_error(y_val, y_pred)\n",
        "        mse_scores.append(mse)\n",
        "\n",
        "    return np.mean(mse_scores)\n",
        "\n",
        "# Function to perform Percentage Split validation\n",
        "def percentage_split_validation(model, X, y, test_size=0.3):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "    return mse\n",
        "\n",
        "# Perform cross-validation and validation on the models\n",
        "k = 5  # K-fold cross-validation: number of folds\n",
        "mse_dt_kfold = k_fold_cross_validation(decision_tree_model, X, y, k)\n",
        "mse_xgboost_kfold = k_fold_cross_validation(xgboost_model, X, y, k)\n",
        "\n",
        "mse_dt_leave_one_out = leave_one_out_cross_validation(decision_tree_model, X, y)\n",
        "mse_xgboost_leave_one_out = leave_one_out_cross_validation(xgboost_model, X, y)\n",
        "\n",
        "mse_dt_percentage_split = percentage_split_validation(decision_tree_model, X, y, test_size=0.3)\n",
        "mse_xgboost_percentage_split = percentage_split_validation(xgboost_model, X, y, test_size=0.3)\n",
        "\n",
        "# Print the mean squared errors for each validation method\n",
        "print(\"Mean Squared Error (Decision Tree) - K-fold Cross-Validation:\", mse_dt_kfold)\n",
        "print(\"Mean Squared Error (XGBoost) - K-fold Cross-Validation:\", mse_xgboost_kfold)\n",
        "\n",
        "print(\"Mean Squared Error (Decision Tree) - Leave-One-Out Cross-Validation:\", mse_dt_leave_one_out)\n",
        "print(\"Mean Squared Error (XGBoost) - Leave-One-Out Cross-Validation:\", mse_xgboost_leave_one_out)\n",
        "\n",
        "print(\"Mean Squared Error (Decision Tree) - Percentage Split Validation:\", mse_dt_percentage_split)\n",
        "print(\"Mean Squared Error (XGBoost) - Percentage Split Validation:\", mse_xgboost_percentage_split)\n"
      ],
      "metadata": {
        "id": "sWdTemhQtN3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**calculate RMSE and R-squared**"
      ],
      "metadata": {
        "id": "_x2lRVAMt5bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_squared_error, r2_score\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse_dt_kfold = np.sqrt(mse_dt_kfold)\n",
        "rmse_xgboost_kfold = np.sqrt(mse_xgboost_kfold)\n",
        "\n",
        "rmse_dt_leave_one_out = np.sqrt(mse_dt_leave_one_out)\n",
        "rmse_xgboost_leave_one_out = np.sqrt(mse_xgboost_leave_one_out)\n",
        "\n",
        "rmse_dt_percentage_split = np.sqrt(mse_dt_percentage_split)\n",
        "rmse_xgboost_percentage_split = np.sqrt(mse_xgboost_percentage_split)\n",
        "\n",
        "# Calculate R-squared\n",
        "r2_dt_kfold = r2_score(y, decision_tree_model.predict(X))\n",
        "r2_xgboost_kfold = r2_score(y, xgboost_model.predict(X))\n",
        "\n",
        "r2_dt_leave_one_out = r2_score(y, decision_tree_model.predict(X))\n",
        "r2_xgboost_leave_one_out = r2_score(y, xgboost_model.predict(X))\n",
        "\n",
        "r2_dt_percentage_split = r2_score(y, decision_tree_model.predict(X))\n",
        "r2_xgboost_percentage_split = r2_score(y, xgboost_model.predict(X))\n",
        "\n",
        "# Print the evaluation results for each model and validation method\n",
        "print(\"Evaluation results for Decision Tree:\")\n",
        "print(\"K-fold Cross-Validation - MSE:\", mse_dt_kfold, \"RMSE:\", rmse_dt_kfold, \"R-squared:\", r2_dt_kfold)\n",
        "print(\"Leave-One-Out Cross-Validation - MSE:\", mse_dt_leave_one_out, \"RMSE:\", rmse_dt_leave_one_out, \"R-squared:\", r2_dt_leave_one_out)\n",
        "print(\"Percentage Split Validation - MSE:\", mse_dt_percentage_split, \"RMSE:\", rmse_dt_percentage_split, \"R-squared:\", r2_dt_percentage_split)\n",
        "\n",
        "print(\"\\nEvaluation results for XGBoost:\")\n",
        "print(\"K-fold Cross-Validation - MSE:\", mse_xgboost_kfold, \"RMSE:\", rmse_xgboost_kfold, \"R-squared:\", r2_xgboost_kfold)\n",
        "print(\"Leave-One-Out Cross-Validation - MSE:\", mse_xgboost_leave_one_out, \"RMSE:\", rmse_xgboost_leave_one_out, \"R-squared:\", r2_xgboost_leave_one_out)\n",
        "print(\"Percentage Split Validation - MSE:\", mse_xgboost_percentage_split, \"RMSE:\", rmse_xgboost_percentage_split, \"R-squared:\", r2_xgboost_percentage_split)\n"
      ],
      "metadata": {
        "id": "o1YF53NawVeo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}